{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
   "metadata": {
    "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch transformers peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008c3139-bd01-4755-805a-84eecd10666a",
   "metadata": {
    "id": "008c3139-bd01-4755-805a-84eecd10666a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes-0.39.1-py3.10.egg/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes-0.39.1-py3.10.egg/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes-0.39.1-py3.10.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so.11.0'), PosixPath('/opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2eea0b0-4a9a-4922-9432-01d5dd8a1864",
   "metadata": {
    "id": "d2eea0b0-4a9a-4922-9432-01d5dd8a1864",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad84b13e-3783-4c86-953e-213c7bc03c5f",
   "metadata": {
    "id": "ad84b13e-3783-4c86-953e-213c7bc03c5f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  32982 MiB |  33102 MiB | 160531 MiB | 127548 MiB |\n",
      "|       from large pool |  32525 MiB |  32751 MiB | 159912 MiB | 127387 MiB |\n",
      "|       from small pool |    457 MiB |    577 MiB |    618 MiB |    160 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  32982 MiB |  33102 MiB | 160531 MiB | 127548 MiB |\n",
      "|       from large pool |  32525 MiB |  32751 MiB | 159912 MiB | 127387 MiB |\n",
      "|       from small pool |    457 MiB |    577 MiB |    618 MiB |    160 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  32982 MiB |  33102 MiB | 160523 MiB | 127540 MiB |\n",
      "|       from large pool |  32525 MiB |  32751 MiB | 159905 MiB | 127380 MiB |\n",
      "|       from small pool |    457 MiB |    577 MiB |    618 MiB |    160 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  33946 MiB |  33946 MiB |  33946 MiB |      0 B   |\n",
      "|       from large pool |  33368 MiB |  33368 MiB |  33368 MiB |      0 B   |\n",
      "|       from small pool |    578 MiB |    578 MiB |    578 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    885 MiB |    885 MiB |  76237 MiB |  75351 MiB |\n",
      "|       from large pool |    843 MiB |    875 MiB |  75705 MiB |  74862 MiB |\n",
      "|       from small pool |     42 MiB |     42 MiB |    531 MiB |    489 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    2964    |    3284    |    5925    |    2961    |\n",
      "|       from large pool |     802    |     804    |    1922    |    1120    |\n",
      "|       from small pool |    2162    |    2482    |    4003    |    1841    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    2964    |    3284    |    5925    |    2961    |\n",
      "|       from large pool |     802    |     804    |    1922    |    1120    |\n",
      "|       from small pool |    2162    |    2482    |    4003    |    1841    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     432    |     432    |     432    |       0    |\n",
      "|       from large pool |     143    |     143    |     143    |       0    |\n",
      "|       from small pool |     289    |     289    |     289    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     190    |     191    |     910    |     720    |\n",
      "|       from large pool |     160    |     160    |     416    |     256    |\n",
      "|       from small pool |      30    |      31    |     494    |     464    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
   "metadata": {
    "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
   "metadata": {
    "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "base_model = \"huggyllama/llama-65b\"\n",
    "lora_weights = \"chansung/alpaca-lora-65b\"\n",
    "cache_directory = \"/mnts/llm3\"\n",
    "\n",
    "prompter = Prompter(\"\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
   "metadata": {
    "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf82657a8fce49a0b297915627071e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to run this with load_8bit=True, you need to run the cp commands from the below two issues:\n",
    "# https://github.com/tloen/alpaca-lora/issues/46\n",
    "# https://github.com/tloen/alpaca-lora/issues/294\n",
    "\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#         base_model,\n",
    "#         load_in_8bit=True,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         device_map=\"auto\",\n",
    "#         cache_dir=cache_directory,\n",
    "#     )\n",
    "# model = PeftModel.from_pretrained(model, lora_weights)\n",
    "\n",
    "# Downgrade to accelerate==0.19.0\n",
    "# https://github.com/artidoro/qlora/issues/151#issuecomment-1629989108\n",
    "\n",
    "load_in_4bit = True\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_directory,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
   "metadata": {
    "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.half()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3591f79b-401e-4d70-82aa-e00b9231b497",
   "metadata": {
    "id": "3591f79b-401e-4d70-82aa-e00b9231b497",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aafe8-a2c7-4866-bd99-23e90e6a4df3",
   "metadata": {
    "id": "518aafe8-a2c7-4866-bd99-23e90e6a4df3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59b0836-8989-4e96-ac2d-9af11f48be72",
   "metadata": {
    "id": "f59b0836-8989-4e96-ac2d-9af11f48be72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.get_device_properties(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
   "metadata": {
    "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
      "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
      "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  5618,\n",
      "           526,   278, 18906,   310,   282, 29765,  6405,   373,   263,   521,\n",
      "           342,  1060, 29899,   764, 29973,    13,    13,  2277, 29937, 13291,\n",
      "         29901,    13]], device='cuda:0')\n",
      "GenerationConfig {\n",
      "  \"temperature\": 0.1,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.75,\n",
      "  \"transformers_version\": \"4.32.0.dev0\"\n",
      "}\n",
      "\n",
      "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the signs of pneumonia on a chest X-ray?\n",
      "\n",
      "### Response:\n",
      "Signs of pneumonia on a chest X-ray include consolidation, which is a white or gray area in the lungs, and air bronchograms, which are air-filled bronchi that appear as dark lines on the X-ray. Other signs include pleural effusion, which is fluid in the pleural space, and pneumothorax, which is air in the pleural space.</s>\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\"): # fix from https://github.com/tloen/alpaca-lora/issues/203\n",
    "    instruction = \"What are the signs of pneumonia on a chest X-ray?\"\n",
    "    prompt = prompter.generate_prompt(instruction, None)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    print(input_ids)\n",
    "\n",
    "    # eval config\n",
    "    temperature = 0.1\n",
    "    top_p = 0.75\n",
    "    top_k = 40\n",
    "    num_beams = 1\n",
    "    max_new_tokens = 128\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    print(generation_config)\n",
    "\n",
    "    # generate_params = {\n",
    "    #     \"input_ids\": input_ids,\n",
    "    #     \"generation_config\": generation_config,\n",
    "    #     \"return_dict_in_generate\": True,\n",
    "    #     \"output_scores\": True,\n",
    "    #     \"max_new_tokens\": max_new_tokens,\n",
    "    # }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d5005-96fe-4413-9277-1cf378ba046d",
   "metadata": {
    "id": "b95d5005-96fe-4413-9277-1cf378ba046d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 + GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
