{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12647841-8b44-477c-a246-5b05dcf56584",
   "metadata": {
    "id": "02064f94-a4aa-4a81-9387-fe0a772add9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install vllm ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4c8b05d-2212-4d15-b72c-1afb9f139645",
   "metadata": {
    "id": "e4c8b05d-2212-4d15-b72c-1afb9f139645",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e801417a-c284-4bab-ae1b-dcefe1930661",
   "metadata": {
    "id": "e801417a-c284-4bab-ae1b-dcefe1930661",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.cuda.device_count()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138de903-e57e-456e-9184-6e41c23c7ed6",
   "metadata": {
    "id": "138de903-e57e-456e-9184-6e41c23c7ed6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 16:34:05,214\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-11-01 16:34:05,657\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 16:34:05 llm_engine.py:72] Initializing an LLM engine with config: model='starmpcc/Asclepius-13B', tokenizer='starmpcc/Asclepius-13B', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-01 16:34:27 llm_engine.py:207] # GPU blocks: 874, # CPU blocks: 327\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from vllm import LLM\n",
    "\n",
    "# ray.init(num_gpus=2) # initialize ray before LLM so that GPUs are defined\n",
    "\n",
    "# llm_mistral = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.1\", tensor_parallel_size=1) \n",
    "llm_asclepius_13b = LLM(model=\"starmpcc/Asclepius-13B\", tensor_parallel_size=1)\n",
    "# llm_asclepius_7b = LLM(model=\"starmpcc/Asclepius-7B\", tensor_parallel_size=1)\n",
    "\n",
    "# mistralai/Mistral-7B-Instruct-v0.1 | GPU blocks: 9257, # CPU blocks: 2048\n",
    "# starmpcc/Asclepius-13B             | GPU blocks: 874,  # CPU blocks: 327\n",
    "# starmpcc/Asclepius-7B              | GPU blocks: 2869, # CPU blocks: 512\n",
    "\n",
    "llm = llm_asclepius_13b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "897097a0-0674-4da9-8450-b7eecf0255fa",
   "metadata": {
    "id": "897097a0-0674-4da9-8450-b7eecf0255fa",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vllm.entrypoints.llm.LLM at 0x7f2fe05df3a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4957f810-4b8b-45bb-ad26-745b6827c153",
   "metadata": {
    "id": "4957f810-4b8b-45bb-ad26-745b6827c153",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7314981315284967\n",
      "145.30938675282002\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=1,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "prompt_base = \"\"\"You are an intelligent clinical languge model.\n",
    "Below is a snippet of patient's discharge summary and a following instruction from healthcare professional.\n",
    "Write a response that appropriately completes the instruction.\n",
    "The response should provide the accurate answer to the instruction, while being concise.\n",
    "\n",
    "[Discharge Summary Begin]\n",
    "{note}\n",
    "[Discharge Summary End]\n",
    "\n",
    "[Instruction Begin]\n",
    "{question}\n",
    "[Instruction End] \n",
    "\"\"\"\n",
    "\n",
    "notes = [\n",
    "    \"Indiction: History : ___F with shortness of breath, cough and recent pneumonia Findings: Heart size remains moderately enlarged. The aorta remains tortuous. Mediastinal and hilar contours are similar with prominence of the right hilum again noted. Pulmonary vasculature is not engorged. Minimal patchy lower lobe opacities, more pronounced on the left, likely reflect areas of atelectasis. No focal consolidation, pleural effusion or pneumothorax is seen. There are no acute osseous abnormalities.\",\n",
    "    \"The patient today completed SBRT for his T-Spine. This patient received a total dose of 30Gy delivered in 5 equal treatments of 6Gy each. Treatment was delivered on 3/29/21, 4/5/21, 4/8/21, 4/14/21 and 4/19/21. We delivered the dose as measured at the 71.4% isodose.\",\n",
    "]\n",
    "questions = [\n",
    "    \"What would be the impression section of the radiology report, given this findings section?\",\n",
    "    \"What is the total dose that patient received?\",\n",
    "]\n",
    "\n",
    "times = []\n",
    "lengths = []\n",
    "for note, question in zip(notes, questions):\n",
    "    prompt = prompt_base.format(note=note, question=question)\n",
    "    s0 = time.perf_counter()\n",
    "    output = llm.generate(prompts=prompt, sampling_params=sampling_params, use_tqdm=False)\n",
    "    s1 = time.perf_counter()\n",
    "    lengths.append((len(prompt), len(output[0].outputs[0].text)))\n",
    "    times.append(s1 - s0)\n",
    "\n",
    "print(sum(times) / len(times))\n",
    "print(sum([output_len / t for (_, output_len), t in zip(lengths, times)]) / len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc813a60-23fa-4ea4-9030-13b329d32a1d",
   "metadata": {
    "id": "dc813a60-23fa-4ea4-9030-13b329d32a1d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[(output_len) / t for (prompt_len, output_len), t in zip(lengths, times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2195beb-5693-458b-88f9-66e86b4572f6",
   "metadata": {
    "id": "e2195beb-5693-458b-88f9-66e86b4572f6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 + GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
