{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
   "metadata": {
    "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (1.12.1)\n",
      "Requirement already satisfied: transformers==4.34.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: accelerate==0.23.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: bitsandbytes==0.41.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (0.14.1)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (0.17.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (4.64.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (1.23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from transformers==4.34.0) (2022.7.9)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from accelerate==0.23.0) (5.9.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2023.9.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.34.0) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers==4.34.0 accelerate==0.23.0 bitsandbytes==0.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008c3139-bd01-4755-805a-84eecd10666a",
   "metadata": {
    "id": "008c3139-bd01-4755-805a-84eecd10666a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad84b13e-3783-4c86-953e-213c7bc03c5f",
   "metadata": {
    "id": "ad84b13e-3783-4c86-953e-213c7bc03c5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
   "metadata": {
    "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
   "metadata": {
    "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95291baf780a4d32a9bf6cbce1b4e018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_in_8bit = True\n",
    "base_model = \"starmpcc/Asclepius-13B\"\n",
    "cache_directory = \"/mnts/llm3\"\n",
    "\n",
    "prompter = Prompter(\"\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=load_in_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_directory,\n",
    ")\n",
    "\n",
    "if not load_in_8bit:\n",
    "    model.half()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
   "metadata": {
    "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "# model.config.bos_token_id = 1\n",
    "# model.config.eos_token_id = 2\n",
    "\n",
    "# model.half()\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "#     model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aafe8-a2c7-4866-bd99-23e90e6a4df3",
   "metadata": {
    "id": "518aafe8-a2c7-4866-bd99-23e90e6a4df3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
   "metadata": {
    "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the signs of pneumonia on a chest X-ray?\n",
      "\n",
      "### Response:\n",
      "The signs of pneumonia on a chest X-ray include a consolidated area in the lung, which appears as a white or cloudy patch. The edges of the patch may be sharp or rounded, and it may be surrounded by a clear or hazy border. The size of the patch can range from a few millimeters to several centimeters. In addition, the presence of a pneumothorax or collapsed lung may also be visible on a chest X-ray.</s><s>\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "with torch.autocast(\"cuda\"):\n",
    "    instruction = \"What are the signs of pneumonia on a chest X-ray?\"\n",
    "    prompt = prompter.generate_prompt(instruction, None)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    # print(input_ids)\n",
    "\n",
    "    # eval config\n",
    "    temperature = 0.2\n",
    "    top_p = 0.75\n",
    "    top_k = 40\n",
    "    num_beams = 1\n",
    "    max_new_tokens = 512\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    # print(generation_config)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e0105-fdbb-4a97-a587-552ba09de54f",
   "metadata": {
    "id": "8a7e0105-fdbb-4a97-a587-552ba09de54f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 + GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
