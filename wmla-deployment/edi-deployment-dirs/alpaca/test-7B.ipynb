{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
   "metadata": {
    "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torch transformers peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008c3139-bd01-4755-805a-84eecd10666a",
   "metadata": {
    "id": "008c3139-bd01-4755-805a-84eecd10666a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda114.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.10-CUDA/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so'), PosixPath('/opt/conda/envs/Python-3.10-CUDA/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
   "metadata": {
    "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
   "metadata": {
    "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c979a21cca444984af620c42ae2e6327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b454a521b8ff4dc7b00be837e69b0706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4b04282bda4536925e9ed09dd68b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "base_model = \"yahma/llama-7b-hf\"\n",
    "lora_weights = \"yahma/alpaca-7b-lora\"\n",
    "cache_directory = \"/mnts/llm\"\n",
    "\n",
    "prompter = Prompter(\"\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
   "metadata": {
    "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717b288a67514c2e9d0a826cefa3a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0b105e3dd140088e4d478f89d6385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/adapter_config.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66e7d1852b7488cb38e6c4ac799cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to run this with load_8bit=True, you need to run the cp commands from the below two issues:\n",
    "# https://github.com/tloen/alpaca-lora/issues/46\n",
    "# https://github.com/tloen/alpaca-lora/issues/294\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=cache_directory,\n",
    "    )\n",
    "model = PeftModel.from_pretrained(model, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
   "metadata": {
    "id": "a0e8a579-36dd-43bd-84b1-4eafa134fe92",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3591f79b-401e-4d70-82aa-e00b9231b497",
   "metadata": {
    "id": "3591f79b-401e-4d70-82aa-e00b9231b497",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaAttention(\n",
       "                (q_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (k_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (v_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (o_proj): Linear8bitLt(\n",
       "                  in_features=4096, out_features=4096, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "                (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm()\n",
       "              (post_attention_layernorm): LlamaRMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
   "metadata": {
    "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
      "         14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
      "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  5618,\n",
      "           526,   278, 18906,   310,   282, 29765,  6405,   373,   263,   521,\n",
      "           342,  1060, 29899,   764, 29973,    13,    13,  2277, 29937, 13291,\n",
      "         29901,    13]], device='cuda:0')\n",
      "GenerationConfig {\n",
      "  \"num_beams\": 4,\n",
      "  \"temperature\": 0.1,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.75,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "<s>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the signs of pneumonia on a chest X-ray?\n",
      "\n",
      "### Response:\n",
      "Pneumonia is an infection of the lungs that can cause inflammation and fluid buildup in the air sacs of the lungs. On a chest X-ray, the signs of pneumonia can include:\n",
      "\n",
      "1. Consolidation: This refers to areas of the lungs where the air sacs are filled with fluid or pus, making them appear white on the X-ray.\n",
      "\n",
      "2. Pleural Effusion: This is the buildup of fluid in the space between the lungs and the chest wall, causing the lungs to appear enlarged\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast(\"cuda\"): # fix from https://github.com/tloen/alpaca-lora/issues/203\n",
    "    instruction = \"What are the signs of pneumonia on a chest X-ray?\"\n",
    "    prompt = prompter.generate_prompt(instruction, None)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    print(input_ids)\n",
    "\n",
    "    # eval config\n",
    "    temperature = 0.1\n",
    "    top_p = 0.75\n",
    "    top_k = 40\n",
    "    num_beams = 4\n",
    "    max_new_tokens = 128\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        num_beams=num_beams,\n",
    "    )\n",
    "    print(generation_config)\n",
    "\n",
    "    # generate_params = {\n",
    "    #     \"input_ids\": input_ids,\n",
    "    #     \"generation_config\": generation_config,\n",
    "    #     \"return_dict_in_generate\": True,\n",
    "    #     \"output_scores\": True,\n",
    "    #     \"max_new_tokens\": max_new_tokens,\n",
    "    # }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d5005-96fe-4413-9277-1cf378ba046d",
   "metadata": {
    "id": "b95d5005-96fe-4413-9277-1cf378ba046d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 + GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
