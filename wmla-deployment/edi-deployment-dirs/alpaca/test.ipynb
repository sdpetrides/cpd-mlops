{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
   "metadata": {
    "id": "ba32fa79-0304-40a6-b10a-e9bba202f66a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (4.30.2)\n",
      "Requirement already satisfied: peft in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (0.20.3)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (0.39.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from peft) (5.8.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2021.10.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.9)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.7.99)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.6.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (8.5.0.96)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.14.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.4.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (58.0.4)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft) (0.37.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.26.4)\n",
      "Requirement already satisfied: lit in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages (from sympy->torch>=1.13.0->peft) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4964593a-8009-4f0e-85b9-4adec5e2f5be",
   "metadata": {
    "id": "756b4b3c-d8d8-4de0-a898-135385a7fe51",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/envs/Python-3.9-CUDA/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 112\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda112.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/opt/conda/envs/Python-3.9-CUDA/lib/libcudart.so.11.0'), PosixPath('/opt/conda/envs/Python-3.9-CUDA/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
   "metadata": {
    "id": "2d28c3c0-d971-49d5-aab1-6e92042fa7ec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class Prompter(object):\n",
    "    __slots__ = (\"template\", \"_verbose\")\n",
    "\n",
    "    def __init__(self, template_name: str = \"\", verbose: bool = False):\n",
    "        self._verbose = verbose\n",
    "        if not template_name:\n",
    "            # Enforce the default here, so the constructor can be called with '' and will not break.\n",
    "            template_name = \"alpaca\"\n",
    "        file_name = osp.join(\"./templates\", f\"{template_name}.json\")\n",
    "        if not osp.exists(file_name):\n",
    "            raise ValueError(f\"Can't read {file_name}\")\n",
    "        with open(file_name) as fp:\n",
    "            self.template = json.load(fp)\n",
    "        if self._verbose:\n",
    "            print(\n",
    "                f\"Using prompt template {template_name}: {self.template['description']}\"\n",
    "            )\n",
    "\n",
    "    def generate_prompt(\n",
    "        self,\n",
    "        instruction: str,\n",
    "        input: Union[None, str] = None,\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "        # returns the full prompt from instruction and optional input\n",
    "        # if a label (=response, =output) is provided, it's also appended.\n",
    "        if input:\n",
    "            res = self.template[\"prompt_input\"].format(\n",
    "                instruction=instruction, input=input\n",
    "            )\n",
    "        else:\n",
    "            res = self.template[\"prompt_no_input\"].format(\n",
    "                instruction=instruction\n",
    "            )\n",
    "        if label:\n",
    "            res = f\"{res}{label}\"\n",
    "        if self._verbose:\n",
    "            print(res)\n",
    "        return res\n",
    "\n",
    "    def get_response(self, output: str) -> str:\n",
    "        return output.split(self.template[\"response_split\"])[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
   "metadata": {
    "id": "6cfcfac4-71c9-4327-a6c1-360bb6b25c10",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a45846f16246ccac1d1ef604933568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acee51ac0ef5473e8fd8b632c12a4bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42975b6aedc452fb09887d17accbfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "cache_directory = \"/mnts/llm3\"\n",
    "\n",
    "# params\n",
    "# base_model = \"huggyllama/llama-65b\"\n",
    "base_model = \"huggyllama/llama-30b\"\n",
    "device = \"cuda\"\n",
    "# lora_weights = \"chansung/alpaca-lora-65b\"\n",
    "lora_weights = \"baseten/alpaca-30b\"\n",
    "\n",
    "prompter = Prompter(\"\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
   "metadata": {
    "id": "cb82af40-b49e-4c83-808b-cc3736db2006",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09fb3fc08857409f8d1474c8f028b54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7055c64de55f41b481b450ec7819d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dbbe435adcd4111902be1b62e466b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049252945cac46aa953180fcf189e5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5c16131d13435c84f2e0bd3c618cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e0ccbaa8dc44af9b03c69d7db113a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bad11e85ab745a1bd17d132b6d0f497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b753b85b5be14473b17fc5820c7ebb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00007.safetensors:   0%|          | 0.00/5.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c40f9cf91a418e8336aa9c72eebb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "unable to mmap 9818304536 bytes from file </mnts/llm3/models--huggyllama--llama-30b/snapshots/2b1edcdb3c7ced7bce6c1aa75c94545777c3118b/model-00001-of-00007.safetensors>: Cannot allocate memory (12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/1000670000/ipykernel_683/960851251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# https://github.com/tloen/alpaca-lora/issues/46\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# https://github.com/tloen/alpaca-lora/issues/294\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model = LlamaForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_8bit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2879\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2881\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2882\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2883\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3212\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshard_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisk_only_shard_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3213\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3214\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3216\u001b[0m                 \u001b[0;31m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/Python-3.9-CUDA/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".safetensors\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_safetensors_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Check format of the archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msafe_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"flax\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unable to mmap 9818304536 bytes from file </mnts/llm3/models--huggyllama--llama-30b/snapshots/2b1edcdb3c7ced7bce6c1aa75c94545777c3118b/model-00001-of-00007.safetensors>: Cannot allocate memory (12)"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "import bitsandbytes\n",
    "\n",
    "load_8bit = True\n",
    "# to run this with load_8bit=True, you need to run the cp commands from the below two issues:\n",
    "# https://github.com/tloen/alpaca-lora/issues/46\n",
    "# https://github.com/tloen/alpaca-lora/issues/294\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=load_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_directory,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=cache_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
   "metadata": {
    "id": "0ae411e3-621f-4705-a7f4-ace3ebb259af",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/1000670000/ipykernel_327/2355735933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b601b4c7-3e5e-490c-b92e-d0f102088f12",
   "metadata": {
    "id": "b601b4c7-3e5e-490c-b92e-d0f102088f12"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 + GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
